# ğŸ“ Mini-GPT é¡¹ç›®å®Œæ•´å­¦ä¹ æŒ‡å—

## ğŸ“š ç›®å½•

1. [ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®æ¦‚è§ˆä¸åŸºç¡€çŸ¥è¯†](#ç¬¬ä¸€éƒ¨åˆ†é¡¹ç›®æ¦‚è§ˆä¸åŸºç¡€çŸ¥è¯†)
2. [ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸åˆ†è¯å™¨](#ç¬¬äºŒéƒ¨åˆ†æ•°æ®å¤„ç†ä¸åˆ†è¯å™¨)
3. [ç¬¬ä¸‰éƒ¨åˆ†ï¼šTransformer æ¨¡å‹æ¶æ„](#ç¬¬ä¸‰éƒ¨åˆ†transformer-æ¨¡å‹æ¶æ„)
4. [ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒæµç¨‹](#ç¬¬å››éƒ¨åˆ†è®­ç»ƒæµç¨‹)
5. [ç¬¬äº”éƒ¨åˆ†ï¼šæ–‡æœ¬ç”Ÿæˆ](#ç¬¬äº”éƒ¨åˆ†æ–‡æœ¬ç”Ÿæˆ)
6. [ç¬¬å…­éƒ¨åˆ†ï¼šå®è·µç»ƒä¹ ](#ç¬¬å…­éƒ¨åˆ†å®è·µç»ƒä¹ )

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®æ¦‚è§ˆä¸åŸºç¡€çŸ¥è¯†

## 1.1 ä»€ä¹ˆæ˜¯ GPTï¼Ÿ

**GPT** = **G**enerative **P**re-trained **T**ransformerï¼ˆç”Ÿæˆå¼é¢„è®­ç»ƒ Transformerï¼‰

### æ ¸å¿ƒæ¦‚å¿µï¼š
- **ç”Ÿæˆå¼**ï¼šæ¨¡å‹å¯ä»¥ç”Ÿæˆæ–°çš„æ–‡æœ¬
- **é¢„è®­ç»ƒ**ï¼šåœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¹ è¯­è¨€æ¨¡å¼
- **Transformer**ï¼šä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

### ç±»æ¯”ç†è§£ï¼š
```
æƒ³è±¡ä½ åœ¨å†™ä½œæ–‡ï¼š
- ä½ çœ‹åˆ°äº†å‰é¢çš„å­—ï¼š"ä»Šå¤©å¤©æ°”å¾ˆ"
- ä½ çš„å¤§è„‘ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ï¼šå¯èƒ½æ˜¯"å¥½"ã€"å†·"ã€"çƒ­"ç­‰
- GPT å°±æ˜¯åšè¿™ä»¶äº‹çš„ AI æ¨¡å‹
```

## 1.2 é¡¹ç›®æ•´ä½“ç»“æ„

```
mini-gpt/
â”‚
â”œâ”€â”€ æ•°æ®å±‚ (Data Layer)
â”‚   â””â”€â”€ data/input.txt          # è®­ç»ƒæ•°æ®ï¼ˆèå£«æ¯”äºšæ–‡æœ¬ï¼‰
â”‚
â”œâ”€â”€ å·¥å…·å±‚ (Utility Layer)
â”‚   â”œâ”€â”€ utils.py                # åˆ†è¯å™¨ï¼ˆæ–‡æœ¬ â†” æ•°å­—ï¼‰
â”‚   â””â”€â”€ config.py               # é…ç½®å‚æ•°
â”‚
â”œâ”€â”€ æ¨¡å‹å±‚ (Model Layer)
â”‚   â””â”€â”€ model.py                # GPT æ¶æ„ï¼ˆæ ¸å¿ƒï¼‰
â”‚
â”œâ”€â”€ è®­ç»ƒå±‚ (Training Layer)
â”‚   â””â”€â”€ train.py                # è®­ç»ƒè„šæœ¬
â”‚
â””â”€â”€ åº”ç”¨å±‚ (Application Layer)
    â””â”€â”€ generate.py             # æ–‡æœ¬ç”Ÿæˆ
```

## 1.3 ä½ éœ€è¦æŒæ¡çš„çŸ¥è¯†åœ°å›¾

```
åŸºç¡€çŸ¥è¯†ï¼ˆå¿…é¡»ï¼‰
â”œâ”€â”€ Python ç¼–ç¨‹
â”‚   â”œâ”€â”€ ç±» (Class) å’Œå¯¹è±¡
â”‚   â”œâ”€â”€ å‡½æ•°å’Œæ¨¡å—
â”‚   â””â”€â”€ åŸºæœ¬æ•°æ®ç»“æ„ï¼ˆåˆ—è¡¨ã€å­—å…¸ï¼‰
â”‚
â”œâ”€â”€ PyTorch åŸºç¡€
â”‚   â”œâ”€â”€ Tensorï¼ˆå¼ é‡ï¼‰æ“ä½œ
â”‚   â”œâ”€â”€ nn.Moduleï¼ˆç¥ç»ç½‘ç»œæ¨¡å—ï¼‰
â”‚   â””â”€â”€ è‡ªåŠ¨æ±‚å¯¼ï¼ˆautogradï¼‰
â”‚
â””â”€â”€ æ•°å­¦åŸºç¡€
    â”œâ”€â”€ çŸ©é˜µä¹˜æ³•
    â”œâ”€â”€ æ¦‚ç‡ï¼ˆsoftmaxï¼‰
    â””â”€â”€ æŸå¤±å‡½æ•°

è¿›é˜¶çŸ¥è¯†ï¼ˆé‡è¦ï¼‰
â”œâ”€â”€ æ·±åº¦å­¦ä¹ 
â”‚   â”œâ”€â”€ ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
â”‚   â”œâ”€â”€ åå‘ä¼ æ’­
â”‚   â””â”€â”€ æ¢¯åº¦ä¸‹é™
â”‚
â””â”€â”€ Transformer æ¶æ„
    â”œâ”€â”€ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰
    â”œâ”€â”€ å¤šå¤´æ³¨æ„åŠ›
    â””â”€â”€ æ®‹å·®è¿æ¥
```

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å¤„ç†ä¸åˆ†è¯å™¨

## 2.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†è¯å™¨ï¼Ÿ

**é—®é¢˜**ï¼šè®¡ç®—æœºä¸ç†è§£æ–‡å­—ï¼Œåªç†è§£æ•°å­—ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåˆ†è¯å™¨ï¼ˆTokenizerï¼‰

```python
# æ–‡æœ¬ â†’ æ•°å­—ï¼ˆç¼–ç ï¼‰
"Hello" â†’ [20, 43, 47, 47, 52]

# æ•°å­— â†’ æ–‡æœ¬ï¼ˆè§£ç ï¼‰
[20, 43, 47, 47, 52] â†’ "Hello"
```

## 2.2 ç†è§£ `utils.py` - å­—ç¬¦çº§åˆ†è¯å™¨

### ç¬¬ä¸€æ­¥ï¼šæ‰“å¼€å¹¶é˜…è¯» utils.py

```bash
cat utils.py
```

### ä»£ç è¯¦è§£ï¼š

```python
class CharTokenizer:
    """å­—ç¬¦çº§åˆ†è¯å™¨"""
    
    def __init__(self, text):
        # æ­¥éª¤1ï¼šè·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦å¹¶æ’åº
        chars = sorted(list(set(text)))
        # ä¾‹ï¼štext = "hello" â†’ chars = ['e', 'h', 'l', 'o']
        
        self.vocab_size = len(chars)
        # vocab_size = 4
        
        # æ­¥éª¤2ï¼šåˆ›å»ºå­—ç¬¦åˆ°æ•°å­—çš„æ˜ å°„
        self.stoi = {ch: i for i, ch in enumerate(chars)}
        # stoi = {'e': 0, 'h': 1, 'l': 2, 'o': 3}
        
        # æ­¥éª¤3ï¼šåˆ›å»ºæ•°å­—åˆ°å­—ç¬¦çš„æ˜ å°„ï¼ˆåå‘ï¼‰
        self.itos = {i: ch for ch, i in self.stoi.items()}
        # itos = {0: 'e', 1: 'h', 2: 'l', 3: 'o'}

    def encode(self, s):
        """æ–‡æœ¬ â†’ æ•°å­—åˆ—è¡¨"""
        return [self.stoi[ch] for ch in s]
        # "hello" â†’ [1, 0, 2, 2, 3]

    def decode(self, ids):
        """æ•°å­—åˆ—è¡¨ â†’ æ–‡æœ¬"""
        return "".join([self.itos[i] for i in ids])
        # [1, 0, 2, 2, 3] â†’ "hello"
```

### åŠ¨æ‰‹å®è·µ1ï¼šæµ‹è¯•åˆ†è¯å™¨

åˆ›å»ºæ–‡ä»¶ `test_tokenizer.py`ï¼š

```python
from utils import CharTokenizer

# æµ‹è¯•æ–‡æœ¬
text = "hello world"

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = CharTokenizer(text)

# æŸ¥çœ‹è¯æ±‡è¡¨
print("å­—ç¬¦é›†:", sorted(list(set(text))))
print("è¯æ±‡è¡¨å¤§å°:", tokenizer.vocab_size)
print("\nå­—ç¬¦â†’æ•°å­—æ˜ å°„ (stoi):")
print(tokenizer.stoi)

# ç¼–ç 
encoded = tokenizer.encode("hello")
print("\nç¼–ç  'hello':", encoded)

# è§£ç 
decoded = tokenizer.decode(encoded)
print("è§£ç å›æ¥:", decoded)
```

**è¿è¡Œ**ï¼š
```bash
python3 test_tokenizer.py
```

### æ€è€ƒé¢˜ï¼š
1. ä¸ºä»€ä¹ˆè¦å¯¹å­—ç¬¦è¿›è¡Œæ’åºï¼Ÿ
2. å¦‚æœæ–‡æœ¬ä¸­å‡ºç°äº†è®­ç»ƒæ—¶æ²¡è§è¿‡çš„å­—ç¬¦ä¼šæ€æ ·ï¼Ÿ
3. å­—ç¬¦çº§åˆ†è¯ vs è¯çº§åˆ†è¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šTransformer æ¨¡å‹æ¶æ„

## 3.1 æ•´ä½“æ¶æ„å›¾

```
è¾“å…¥æ–‡æœ¬: "Hello"
    â†“
[ç¼–ç ] â†’ [20, 43, 47, 47, 52]
    â†“
[Token Embedding] â†’ æ¯ä¸ªæ•°å­—å˜æˆä¸€ä¸ªå‘é‡
    â†“
[Position Embedding] â†’ åŠ ä¸Šä½ç½®ä¿¡æ¯
    â†“
[Transformer Block 1]
    â”œâ”€â”€ Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰
    â”œâ”€â”€ Residual Connectionï¼ˆæ®‹å·®è¿æ¥ï¼‰
    â”œâ”€â”€ Layer Normï¼ˆå±‚å½’ä¸€åŒ–ï¼‰
    â”œâ”€â”€ Feed-Forward Networkï¼ˆå‰é¦ˆç½‘ç»œï¼‰
    â””â”€â”€ Residual Connection
    â†“
[Transformer Block 2]
    ... (é‡å¤ 6 æ¬¡)
    â†“
[Transformer Block 6]
    â†“
[Layer Norm]
    â†“
[Linear Layer] â†’ é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡
    â†“
è¾“å‡º: æ¯ä¸ªå¯èƒ½å­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ
```

## 3.2 æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

### 3.2.1 ä»€ä¹ˆæ˜¯ Embeddingï¼ˆåµŒå…¥ï¼‰ï¼Ÿ

**é—®é¢˜**ï¼šæ•°å­— `20` æœ¬èº«æ²¡æœ‰æ„ä¹‰ã€‚

**è§£å†³**ï¼šå°†æ¯ä¸ªæ•°å­—æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´å‘é‡ã€‚

```python
# ç®€åŒ–ç¤ºä¾‹
æ•°å­— 20 â†’ [0.5, -0.3, 0.8, 0.1, ...]  # 384ç»´å‘é‡
æ•°å­— 43 â†’ [0.2, 0.7, -0.5, 0.9, ...]  # 384ç»´å‘é‡
```

### ç±»æ¯”ï¼š
```
æŠŠæ¯ä¸ªå­—ç¬¦æƒ³è±¡æˆä¸€ä¸ªäºº
æ¯ä¸ªäººæœ‰å¾ˆå¤šç‰¹å¾ï¼ˆèº«é«˜ã€ä½“é‡ã€å¹´é¾„...ï¼‰
è¿™äº›ç‰¹å¾å°±æ˜¯å‘é‡çš„å„ä¸ªç»´åº¦
```

### ä»£ç ä½ç½®ï¼ˆmodel.pyï¼‰ï¼š
```python
self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
# vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆ65ä¸ªå­—ç¬¦ï¼‰
# n_embd: æ¯ä¸ªå­—ç¬¦ç”¨å¤šå°‘ç»´è¡¨ç¤ºï¼ˆ384ç»´ï¼‰
```

### 3.2.2 ä»€ä¹ˆæ˜¯ Position Embeddingï¼ˆä½ç½®åµŒå…¥ï¼‰ï¼Ÿ

**é—®é¢˜**ï¼šæ¨¡å‹éœ€è¦çŸ¥é“å­—ç¬¦çš„é¡ºåºã€‚

```
"Hello" å’Œ "olleH" åº”è¯¥ä¸åŒ
ä½†å¦‚æœåªçœ‹å­—ç¬¦æœ¬èº«ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ†é¡ºåº
```

**è§£å†³**ï¼šç»™æ¯ä¸ªä½ç½®ä¸€ä¸ªç‹¬ç‰¹çš„å‘é‡ã€‚

```python
ä½ç½® 0 â†’ [0.1, 0.2, 0.3, ...]
ä½ç½® 1 â†’ [0.4, 0.5, 0.6, ...]
ä½ç½® 2 â†’ [0.7, 0.8, 0.9, ...]
```

### æœ€ç»ˆè¾“å…¥ï¼š
```
Token Embedding + Position Embedding

"H" åœ¨ä½ç½®0:
  [0.5, -0.3, 0.8, ...]  (token)
+ [0.1,  0.2, 0.3, ...]  (position)
= [0.6, -0.1, 1.1, ...]  (æœ€ç»ˆè¾“å…¥)
```

## 3.3 æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰- æ ¸å¿ƒä¸­çš„æ ¸å¿ƒ

### 3.3.1 ç›´è§‚ç†è§£

**åœºæ™¯**ï¼šä½ åœ¨å†™"ä»Šå¤©å¤©æ°”å¾ˆ____"

ä½ çš„å¤§è„‘ä¼šï¼š
1. **çœ‹** å‰é¢çš„å­—ï¼š"ä»Šå¤©"ã€"å¤©æ°”"ã€"å¾ˆ"
2. **å…³æ³¨** æœ€é‡è¦çš„ä¿¡æ¯ï¼ˆ"å¤©æ°”"ï¼‰
3. **é¢„æµ‹** ä¸‹ä¸€ä¸ªå­—ï¼ˆ"å¥½"ã€"å†·"ç­‰ï¼‰

**æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯è®©æ¨¡å‹å­¦ä¼š"å…³æ³¨"ï¼**

### 3.3.2 æ³¨æ„åŠ›çš„ä¸‰ä¸ªæ­¥éª¤

#### æ­¥éª¤1ï¼šè®¡ç®— Query, Key, Value (Q, K, V)

```python
# ç®€åŒ–ç†è§£
Query (æŸ¥è¯¢):  "æˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
Key (é”®):      "æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
Value (å€¼):    "ä¿¡æ¯çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
```

#### å®é™…ä¾‹å­ï¼š
```
å¥å­ï¼š"The cat sat on the mat"
å½“é¢„æµ‹ "mat" åé¢çš„è¯æ—¶ï¼š

Query: "mat" åœ¨é—®ï¼š"æˆ‘åº”è¯¥å…³æ³¨ä»€ä¹ˆï¼Ÿ"
Key: æ¯ä¸ªè¯è¯´ï¼š"æˆ‘æ˜¯ The/cat/sat/on/the/mat"
Value: æ¯ä¸ªè¯çš„å®é™…å«ä¹‰å‘é‡
```

#### æ­¥éª¤2ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

```python
# å…¬å¼
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd) @ V

# åˆ†è§£ï¼š
1. Q @ K^T: è®¡ç®—ç›¸ä¼¼åº¦
2. / âˆšd: ç¼©æ”¾ï¼ˆé¿å…æ•°å€¼è¿‡å¤§ï¼‰
3. softmax: è½¬æ¢æˆæ¦‚ç‡ï¼ˆå’Œä¸º1ï¼‰
4. @ V: åŠ æƒæ±‚å’Œ
```

#### æ­¥éª¤3ï¼šåº”ç”¨ Causal Maskï¼ˆå› æœæ©ç ï¼‰

**é—®é¢˜**ï¼šè®­ç»ƒæ—¶ä¸èƒ½"ä½œå¼Š"ï¼ˆçœ‹åˆ°æœªæ¥çš„è¯ï¼‰

```
é¢„æµ‹ä½ç½® 2 çš„è¯æ—¶ï¼Œåªèƒ½çœ‹ä½ç½® 0, 1, 2
ä¸èƒ½çœ‹ä½ç½® 3, 4, 5ï¼ˆé‚£æ˜¯æœªæ¥ï¼‰
```

**è§£å†³**ï¼šä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µ

```python
# torch.tril åˆ›å»ºä¸‹ä¸‰è§’çŸ©é˜µ
[[1, 0, 0, 0],   # ä½ç½®0åªèƒ½çœ‹è‡ªå·±
 [1, 1, 0, 0],   # ä½ç½®1å¯ä»¥çœ‹0,1
 [1, 1, 1, 0],   # ä½ç½®2å¯ä»¥çœ‹0,1,2
 [1, 1, 1, 1]]   # ä½ç½®3å¯ä»¥çœ‹0,1,2,3
```

### 3.3.3 ä»£ç è¯¦è§£ï¼ˆmodel.py çš„ Head ç±»ï¼‰

```python
class Head(nn.Module):
    """å•ä¸ªæ³¨æ„åŠ›å¤´"""
    
    def __init__(self, n_embd, head_size, block_size, dropout=0.1):
        super().__init__()
        # åˆ›å»º Q, K, V çš„çº¿æ€§å˜æ¢å±‚
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        
        # æ³¨å†Œå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x çš„å½¢çŠ¶: (batch, time, channels)
        B, T, C = x.shape
        
        # è®¡ç®— Q, K, V
        k = self.key(x)   # (B, T, head_size)
        q = self.query(x) # (B, T, head_size)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # q @ k^T: (B, T, head_size) @ (B, head_size, T) â†’ (B, T, T)
        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # ç¼©æ”¾
        
        # åº”ç”¨å› æœæ©ç 
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        
        # softmax è½¬æ¢æˆæ¦‚ç‡
        wei = F.softmax(wei, dim=-1)  # (B, T, T)
        wei = self.dropout(wei)
        
        # åŠ æƒæ±‚å’Œ
        v = self.value(x)  # (B, T, head_size)
        out = wei @ v      # (B, T, T) @ (B, T, head_size) â†’ (B, T, head_size)
        
        return out
```

### åŠ¨æ‰‹å®è·µ2ï¼šå¯è§†åŒ–æ³¨æ„åŠ›

åˆ›å»º `visualize_attention.py`ï¼š

```python
import torch
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡çŸ©é˜µ
seq_len = 8
attention = torch.tril(torch.ones(seq_len, seq_len))
attention = attention / attention.sum(dim=1, keepdim=True)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.imshow(attention, cmap='Blues')
plt.colorbar()
plt.title('Causal Attention Pattern')
plt.xlabel('Key Position')
plt.ylabel('Query Position')
plt.savefig('attention_pattern.png')
print("å·²ä¿å­˜åˆ° attention_pattern.png")
```

## 3.4 å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªå¤´ï¼Ÿ

**ç±»æ¯”**ï¼š
```
ä¸€ä¸ªäººçœ‹é—®é¢˜ï¼Œè§†è§’å•ä¸€
å¤šä¸ªäººï¼ˆå¤šä¸ªå¤´ï¼‰çœ‹é—®é¢˜ï¼Œè§†è§’å…¨é¢

å¤´1: å…³æ³¨è¯­æ³•
å¤´2: å…³æ³¨è¯­ä¹‰
å¤´3: å…³æ³¨ä¸Šä¸‹æ–‡
...
```

### ä»£ç ï¼ˆmodel.pyï¼‰ï¼š

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, n_embd, num_heads, block_size, dropout=0.1):
        super().__init__()
        head_size = n_embd // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        # åˆ›å»ºå¤šä¸ªæ³¨æ„åŠ›å¤´
        self.heads = nn.ModuleList([
            Head(n_embd, head_size, block_size, dropout) 
            for _ in range(num_heads)
        ])
        
        # è¾“å‡ºæŠ•å½±
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—ï¼Œç„¶åæ‹¼æ¥
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out
```

## 3.5 å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Networkï¼‰

### ä½œç”¨ï¼š
åœ¨æ³¨æ„åŠ›ä¹‹åï¼Œè¿›ä¸€æ­¥å¤„ç†ä¿¡æ¯

### ç»“æ„ï¼š
```
è¾“å…¥ (384ç»´)
  â†“
çº¿æ€§å±‚1 â†’ 1536ç»´ (æ‰©å±•4å€)
  â†“
ReLU æ¿€æ´»
  â†“
çº¿æ€§å±‚2 â†’ 384ç»´ (å‹ç¼©å›æ¥)
  â†“
è¾“å‡º (384ç»´)
```

### ä»£ç ï¼š

```python
class FeedForward(nn.Module):
    def __init__(self, n_embd, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # æ‰©å±•
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),  # å‹ç¼©
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        return self.net(x)
```

## 3.6 Transformer Blockï¼ˆç»„åˆèµ·æ¥ï¼‰

### ç»“æ„ï¼š
```
è¾“å…¥
  â†“
LayerNorm â†’ MultiHeadAttention â†’ æ®‹å·®è¿æ¥
  â†“
LayerNorm â†’ FeedForward â†’ æ®‹å·®è¿æ¥
  â†“
è¾“å‡º
```

### ä»€ä¹ˆæ˜¯æ®‹å·®è¿æ¥ï¼Ÿ

**é—®é¢˜**ï¼šæ·±å±‚ç½‘ç»œéš¾ä»¥è®­ç»ƒ

**è§£å†³**ï¼šç›´æ¥åŠ ä¸ŠåŸå§‹è¾“å…¥

```python
# æ²¡æœ‰æ®‹å·®è¿æ¥
output = attention(x)

# æœ‰æ®‹å·®è¿æ¥
output = x + attention(x)
#        â†‘   â†‘
#        åŸ  æ–°ä¿¡æ¯
#        å§‹
#        è¾“
#        å…¥
```

### ä»£ç ï¼š

```python
class Block(nn.Module):
    def __init__(self, n_embd, num_heads, block_size, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(n_embd)
        self.attn = MultiHeadAttention(n_embd, num_heads, block_size, dropout)
        self.ln2 = nn.LayerNorm(n_embd)
        self.ffwd = FeedForward(n_embd, dropout)
    
    def forward(self, x):
        # æ³¨æ„åŠ› + æ®‹å·®
        x = x + self.attn(self.ln1(x))
        
        # å‰é¦ˆ + æ®‹å·®
        x = x + self.ffwd(self.ln2(x))
        
        return x
```

---

# ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒæµç¨‹

## 4.1 è®­ç»ƒçš„æœ¬è´¨

**ç›®æ ‡**ï¼šæ•™ä¼šæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦

```
è¾“å…¥:  "Hell"
ç›®æ ‡:  "o"

è¾“å…¥:  "Hello worl"
ç›®æ ‡:  "d"
```

## 4.2 æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

**ä½œç”¨**ï¼šè¡¡é‡æ¨¡å‹çš„é¢„æµ‹æœ‰å¤š"é”™"

### Cross-Entropy Lossï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰

```python
# æ¨¡å‹é¢„æµ‹ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰
é¢„æµ‹: {'a': 0.1, 'b': 0.2, 'c': 0.05, 'd': 0.6, 'e': 0.05}
çœŸå®: 'd'

# å¥½çš„é¢„æµ‹ï¼šç»™æ­£ç¡®ç­”æ¡ˆé«˜æ¦‚ç‡
é¢„æµ‹: {'d': 0.9, ...}  # Loss å¾ˆå° âœ“

# åçš„é¢„æµ‹ï¼šç»™é”™è¯¯ç­”æ¡ˆé«˜æ¦‚ç‡
é¢„æµ‹: {'a': 0.9, ...}  # Loss å¾ˆå¤§ âœ—
```

### ä»£ç ï¼ˆmodel.pyï¼‰ï¼š

```python
def forward(self, idx, targets=None):
    # ... å‰å‘ä¼ æ’­ ...
    
    if targets is None:
        loss = None
    else:
        B, T, C = logits.shape
        logits = logits.view(B * T, C)
        targets = targets.view(B * T)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(logits, targets)
    
    return logits, loss
```

## 4.3 è®­ç»ƒå¾ªç¯è¯¦è§£

### å®Œæ•´æµç¨‹ï¼š

```python
for iteration in range(max_iters):
    # 1. è·å–ä¸€æ‰¹è®­ç»ƒæ•°æ®
    x, y = get_batch('train')
    # x: è¾“å…¥åºåˆ— (batch_size, block_size)
    # y: ç›®æ ‡åºåˆ— (batch_size, block_size)
    
    # 2. å‰å‘ä¼ æ’­ï¼šé¢„æµ‹
    logits, loss = model(x, y)
    
    # 3. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
    optimizer.zero_grad()  # æ¸…ç©ºä¸Šä¸€æ¬¡çš„æ¢¯åº¦
    loss.backward()        # è®¡ç®—æ¢¯åº¦
    
    # 4. æ›´æ–°å‚æ•°
    optimizer.step()
    
    # 5. å®šæœŸè¯„ä¼°
    if iteration % eval_interval == 0:
        # åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•
        val_loss = evaluate(model, val_data)
        print(f"Step {iteration}: train loss {loss:.4f}, val loss {val_loss:.4f}")
```

### å…³é”®å‡½æ•°ï¼š`get_batch`

```python
def get_batch(split):
    """ç”Ÿæˆä¸€æ‰¹è®­ç»ƒæ•°æ®"""
    data = train_data if split == 'train' else val_data
    
    # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
    ix = torch.randint(len(data) - block_size, (batch_size,))
    
    # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    
    return x, y
```

### ç¤ºä¾‹ï¼š

```
åŸå§‹æ•°æ®: "Hello world"
ç¼–ç : [20, 43, 47, 47, 52, 1, 58, 52, 55, 47, 42]

block_size = 4

æ‰¹æ¬¡1:
  x = [20, 43, 47, 47]  â†’ "Hell"
  y = [43, 47, 47, 52]  â†’ "ello"

æ‰¹æ¬¡2:
  x = [52, 1, 58, 52]   â†’ "o wo"
  y = [1, 58, 52, 55]   â†’ " wor"
```

## 4.4 ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰

### AdamW ä¼˜åŒ–å™¨

**ä½œç”¨**ï¼šæ™ºèƒ½åœ°è°ƒæ•´å­¦ä¹ é€Ÿåº¦

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
```

**å‚æ•°æ›´æ–°å…¬å¼ï¼ˆç®€åŒ–ï¼‰**ï¼š
```
æ–°å‚æ•° = æ—§å‚æ•° - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
```

### å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰

```
å­¦ä¹ ç‡å¤ªå¤§ â†’ ä¸ç¨³å®šï¼Œè·³æ¥è·³å»
å­¦ä¹ ç‡å¤ªå° â†’ è®­ç»ƒå¤ªæ…¢
```

é»˜è®¤å€¼ï¼š`3e-4` (0.0003) æ˜¯ä¸ªä¸é”™çš„èµ·ç‚¹

---

# ç¬¬äº”éƒ¨åˆ†ï¼šæ–‡æœ¬ç”Ÿæˆ

## 5.1 ç”Ÿæˆçš„æœ¬è´¨

**è‡ªå›å½’ç”Ÿæˆ**ï¼šä¸€ä¸ªå­—ä¸€ä¸ªå­—åœ°ç”Ÿæˆ

```
æ­¥éª¤1: è¾“å…¥ "H"     â†’ é¢„æµ‹ "e"
æ­¥éª¤2: è¾“å…¥ "He"    â†’ é¢„æµ‹ "l"
æ­¥éª¤3: è¾“å…¥ "Hel"   â†’ é¢„æµ‹ "l"
æ­¥éª¤4: è¾“å…¥ "Hell"  â†’ é¢„æµ‹ "o"
...
```

## 5.2 é‡‡æ ·ç­–ç•¥

### 5.2.1 Temperatureï¼ˆæ¸©åº¦ï¼‰

**æ§åˆ¶éšæœºæ€§**

```python
logits = logits / temperature

temperature = 0.1  â†’ ä¿å®ˆï¼ˆæ€»é€‰æœ€å¯èƒ½çš„ï¼‰
temperature = 1.0  â†’ å¹³è¡¡
temperature = 2.0  â†’ åˆ›æ–°ï¼ˆæ›´éšæœºï¼‰
```

### ç¤ºä¾‹ï¼š

```
åŸå§‹æ¦‚ç‡: {'a': 0.5, 'b': 0.3, 'c': 0.2}

temperature = 0.5 (æ›´ç¡®å®š):
â†’ {'a': 0.8, 'b': 0.15, 'c': 0.05}

temperature = 2.0 (æ›´éšæœº):
â†’ {'a': 0.4, 'b': 0.35, 'c': 0.25}
```

### 5.2.2 Top-K é‡‡æ ·

**åªä»æœ€å¯èƒ½çš„ K ä¸ªå€™é€‰ä¸­é€‰æ‹©**

```python
# top_k = 5
åŸå§‹: 65ä¸ªå­—ç¬¦éƒ½å¯èƒ½è¢«é€‰ä¸­
top_k: åªä»æœ€å¯èƒ½çš„5ä¸ªå­—ç¬¦ä¸­é€‰

å¥½å¤„ï¼šé¿å…é€‰åˆ°å¾ˆä¸åˆç†çš„å­—ç¬¦
```

### ä»£ç ï¼ˆmodel.py çš„ generate æ–¹æ³•ï¼‰ï¼š

```python
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
    for _ in range(max_new_tokens):
        # 1. è£å‰ªä¸Šä¸‹æ–‡ï¼ˆåªä¿ç•™æœ€å block_size ä¸ªï¼‰
        idx_cond = idx[:, -self.block_size:]
        
        # 2. å‰å‘ä¼ æ’­
        logits, _ = self(idx_cond)
        
        # 3. åªçœ‹æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
        logits = logits[:, -1, :] / temperature
        
        # 4. Top-K è¿‡æ»¤
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # 5. è½¬æ¢æˆæ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        
        # 6. é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        idx_next = torch.multinomial(probs, num_samples=1)
        
        # 7. æ‹¼æ¥åˆ°åºåˆ—
        idx = torch.cat((idx, idx_next), dim=1)
    
    return idx
```

---

# ç¬¬å…­éƒ¨åˆ†ï¼šå®è·µç»ƒä¹ 

## ç»ƒä¹ 1ï¼šç†è§£åˆ†è¯å™¨

**ä»»åŠ¡**ï¼šå®ç°ä¸€ä¸ªè¯çº§åˆ†è¯å™¨

```python
class WordTokenizer:
    def __init__(self, text):
        # TODO: æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œåˆ›å»ºè¯æ±‡è¡¨
        pass
    
    def encode(self, text):
        # TODO: æ–‡æœ¬ â†’ æ•°å­—åˆ—è¡¨
        pass
    
    def decode(self, ids):
        # TODO: æ•°å­—åˆ—è¡¨ â†’ æ–‡æœ¬
        pass

# æµ‹è¯•
text = "hello world hello"
tokenizer = WordTokenizer(text)
print(tokenizer.encode("hello"))  # åº”è¯¥è¾“å‡º [0] æˆ– [1]
```

## ç»ƒä¹ 2ï¼šå¯è§†åŒ–Embedding

```python
import torch
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# åˆ›å»ºç®€å•çš„embedding
vocab_size = 10
embed_dim = 50
embedding = nn.Embedding(vocab_size, embed_dim)

# è·å–æ‰€æœ‰embeddingå‘é‡
all_embeddings = embedding.weight.detach().numpy()

# é™ç»´åˆ°2D
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(all_embeddings)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
for i in range(vocab_size):
    plt.annotate(str(i), (embeddings_2d[i, 0], embeddings_2d[i, 1]))
plt.title('Token Embeddings (2D)')
plt.savefig('embeddings.png')
```

## ç»ƒä¹ 3ï¼šä¿®æ”¹æ¨¡å‹å‚æ•°

**ä»»åŠ¡**ï¼šåœ¨ `config.py` ä¸­å°è¯•ä¸åŒçš„é…ç½®

```python
# å®éªŒ1: å°æ¨¡å‹ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
n_embd = 128
n_layer = 3
num_heads = 4
max_iters = 1000

# å®éªŒ2: ä¸­ç­‰æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
n_embd = 384
n_layer = 6
num_heads = 6
max_iters = 5000

# å®éªŒ3: å¤§æ¨¡å‹ï¼ˆå¦‚æœGPUè¶³å¤Ÿï¼‰
n_embd = 512
n_layer = 8
num_heads = 8
max_iters = 10000
```

**è§‚å¯Ÿ**ï¼š
- æ¨¡å‹å¤§å°å¯¹è®­ç»ƒæ—¶é—´çš„å½±å“
- æ¨¡å‹å¤§å°å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“
- Loss ä¸‹é™çš„é€Ÿåº¦

## ç»ƒä¹ 4ï¼šè‡ªå®šä¹‰æ•°æ®é›†

**ä»»åŠ¡**ï¼šç”¨è‡ªå·±çš„æ–‡æœ¬è®­ç»ƒæ¨¡å‹

```bash
# 1. å‡†å¤‡ä½ çš„æ–‡æœ¬æ–‡ä»¶
echo "ä½ çš„ä¸­æ–‡æ–‡æœ¬æˆ–è‹±æ–‡æ–‡æœ¬" > data/my_text.txt

# 2. ä¿®æ”¹ train.py
# æŠŠ 'data/input.txt' æ”¹æˆ 'data/my_text.txt'

# 3. è®­ç»ƒ
python3 train.py

# 4. ç”Ÿæˆ
python3 generate.py --prompt "ä½ çš„æç¤ºè¯"
```

## ç»ƒä¹ 5ï¼šå®ç°å­¦ä¹ ç‡è°ƒåº¦å™¨

```python
# åœ¨ train.py ä¸­æ·»åŠ 
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=max_iters)

for iter in range(max_iters):
    # ... è®­ç»ƒä»£ç  ...
    optimizer.step()
    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
    
    if iter % 100 == 0:
        current_lr = scheduler.get_last_lr()[0]
        print(f"Iteration {iter}, LR: {current_lr:.6f}")
```

---

# å­¦ä¹ è·¯çº¿å›¾

## ç¬¬1å‘¨ï¼šåŸºç¡€ç†è§£

- [ ] é˜…è¯»å¹¶ç†è§£ utils.py
- [ ] è¿è¡Œ test_tokenizer.py
- [ ] ç†è§£ Embedding çš„æ¦‚å¿µ
- [ ] å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µ

## ç¬¬2å‘¨ï¼šæ¨¡å‹æ¶æ„

- [ ] é€è¡Œé˜…è¯» model.py
- [ ] ç†è§£å•å¤´æ³¨æ„åŠ›æœºåˆ¶
- [ ] ç†è§£å¤šå¤´æ³¨æ„åŠ›
- [ ] ç†è§£ Transformer Block

## ç¬¬3å‘¨ï¼šè®­ç»ƒä¸ç”Ÿæˆ

- [ ] ç†è§£è®­ç»ƒå¾ªç¯
- [ ] è¿è¡Œå®Œæ•´è®­ç»ƒ
- [ ] å®éªŒä¸åŒçš„ç”Ÿæˆå‚æ•°
- [ ] è§‚å¯Ÿ loss æ›²çº¿

## ç¬¬4å‘¨ï¼šå®éªŒä¸æ”¹è¿›

- [ ] å°è¯•ä¸åŒæ¨¡å‹å¤§å°
- [ ] ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†
- [ ] å®ç°å­¦ä¹ ç‡è°ƒåº¦
- [ ] æ·»åŠ  wandb æ—¥å¿—

---

# è°ƒè¯•æŠ€å·§

## 1. æ‰“å° Tensor å½¢çŠ¶

```python
def forward(self, x):
    print(f"Input shape: {x.shape}")
    x = self.layer1(x)
    print(f"After layer1: {x.shape}")
    # ... ç»§ç»­
```

## 2. æ£€æŸ¥æ¢¯åº¦

```python
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm().item():.4f}")
    else:
        print(f"{name}: NO GRADIENT!")
```

## 3. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

```python
import matplotlib.pyplot as plt

train_losses = []
val_losses = []

for iter in range(max_iters):
    # è®­ç»ƒ...
    train_losses.append(train_loss)
    val_losses.append(val_loss)

# ç»˜åˆ¶
plt.plot(train_losses, label='Train')
plt.plot(val_losses, label='Val')
plt.legend()
plt.savefig('loss_curve.png')
```

---

# å¸¸è§é—®é¢˜ FAQ

## Q1: ä¸ºä»€ä¹ˆ loss ä¸ä¸‹é™ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡å¤ªå¤§æˆ–å¤ªå°
- æ¨¡å‹å¤ªå°ï¼Œå®¹é‡ä¸å¤Ÿ
- æ•°æ®æœ‰é—®é¢˜
- æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

**è§£å†³æ–¹æ³•**ï¼š
```python
# 1. è°ƒæ•´å­¦ä¹ ç‡
learning_rate = 1e-4  # è¯•è¯•æ›´å°çš„

# 2. æ·»åŠ æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

# 3. æ£€æŸ¥æ•°æ®
print(f"Train data: {train_data[:100]}")
```

## Q2: ç”Ÿæˆçš„æ–‡æœ¬æ˜¯ä¹±ç ï¼Ÿ

**åŸå› **ï¼š
- è®­ç»ƒä¸å¤Ÿå……åˆ†
- Temperature è®¾ç½®ä¸å½“

**è§£å†³**ï¼š
```bash
# 1. è®­ç»ƒæ›´ä¹…
max_iters = 10000

# 2. è°ƒæ•´ temperature
python3 generate.py --temperature 0.7  # æ›´ä¿å®ˆ
```

## Q3: å†…å­˜ä¸å¤Ÿï¼Ÿ

**è§£å†³**ï¼š
```python
# å‡å° batch_size
batch_size = 32  # ä» 64 å‡åˆ° 32

# å‡å° block_size
block_size = 128  # ä» 256 å‡åˆ° 128

# å‡å°æ¨¡å‹
n_embd = 256
n_layer = 4
```

---

# è¿›é˜¶å­¦ä¹ èµ„æº

## è®ºæ–‡
1. **Attention Is All You Need** (åŸå§‹ Transformer)
2. **Language Models are Unsupervised Multitask Learners** (GPT-2)

## æ•™ç¨‹
1. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
2. [Andrej Karpathy's YouTube: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)

## ä»£ç 
1. [nanoGPT](https://github.com/karpathy/nanoGPT) - æœ¬é¡¹ç›®çš„çµæ„Ÿæ¥æº

---

# æ€»ç»“

## ä½ å·²ç»æŒæ¡çš„æŠ€èƒ½ï¼š

âœ… **æ•°æ®å¤„ç†**
- å­—ç¬¦çº§åˆ†è¯
- æ–‡æœ¬ç¼–ç /è§£ç 

âœ… **æ¨¡å‹æ¶æ„**
- Embeddingï¼ˆToken + Positionï¼‰
- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- å¤šå¤´æ³¨æ„åŠ›
- å‰é¦ˆç½‘ç»œ
- Transformer Block

âœ… **è®­ç»ƒæµç¨‹**
- æŸå¤±å‡½æ•°
- æ¢¯åº¦ä¸‹é™
- å‚æ•°æ›´æ–°

âœ… **æ–‡æœ¬ç”Ÿæˆ**
- è‡ªå›å½’ç”Ÿæˆ
- Temperature é‡‡æ ·
- Top-K é‡‡æ ·

## ä¸‹ä¸€æ­¥å»ºè®®ï¼š

1. **å®è·µï¼Œå®è·µï¼Œå†å®è·µï¼**
   - åœ¨ä¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒ
   - è°ƒæ•´è¶…å‚æ•°
   - è§‚å¯Ÿæ¨¡å‹è¡Œä¸º

2. **é˜…è¯»ä»£ç **
   - æ¯å¤©è¯»ä¸€ç‚¹ model.py
   - ç†è§£æ¯ä¸€è¡Œçš„ä½œç”¨

3. **åšå®éªŒ**
   - æ”¹åŠ¨ä»£ç ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ
   - è®°å½•ä½ çš„å‘ç°

4. **åˆ†äº«**
   - å†™åšå®¢è®°å½•å­¦ä¹ è¿‡ç¨‹
   - åœ¨ GitHub ä¸Šåˆ†äº«ä½ çš„æ”¹è¿›

---

**æ­å–œä½ ï¼ä½ ç°åœ¨æ‹¥æœ‰æ„å»ºè‡ªå·±çš„è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†äº†ï¼ğŸ‰**

Keep learning, keep building! ğŸ’ª
