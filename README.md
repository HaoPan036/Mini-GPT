# Mini-GPT: A Lightweight GPT-like Model from Scratch in PyTorch

A minimal, educational implementation of a GPT-style decoder-only Transformer built entirely from scratch using PyTorch. Inspired by Andrej Karpathy's nanoGPT, this project demonstrates the core concepts of modern language models in a clean, understandable codebase.

## ğŸš€ Project Summary

This project implements a character-level language model trained on the Tiny Shakespeare dataset. It includes:
- Complete Transformer architecture (multi-head self-attention, feed-forward networks, residual connections)
- Character-level tokenization
- Full training pipeline with evaluation
- Autoregressive text generation with temperature and top-k sampling
- Model checkpointing and loading

## âœ¨ Features

- **Decoder-Only Transformer**: Implements the GPT architecture with causal self-attention
- **Multi-Head Attention**: Parallel attention heads with learned Q, K, V projections
- **Positional Encoding**: Learned positional embeddings for sequence modeling
- **Pre-Layer Normalization**: Modern transformer architecture with Pre-LN
- **Residual Connections**: Skip connections for stable training
- **Character-Level Tokenizer**: Simple and interpretable tokenization
- **Autoregressive Generation**: Sample text with temperature and top-k controls
- **Checkpointing**: Save and load trained models

## ğŸ“Š Model Architecture

```
GPTModel
â”œâ”€â”€ Token Embedding (vocab_size â†’ n_embd)
â”œâ”€â”€ Position Embedding (block_size â†’ n_embd)
â”œâ”€â”€ Transformer Blocks (Ã— n_layer)
â”‚   â”œâ”€â”€ LayerNorm
â”‚   â”œâ”€â”€ Multi-Head Self-Attention
â”‚   â”‚   â”œâ”€â”€ Q, K, V Projections
â”‚   â”‚   â”œâ”€â”€ Causal Masking (torch.tril)
â”‚   â”‚   â””â”€â”€ Output Projection
â”‚   â”œâ”€â”€ Residual Connection
â”‚   â”œâ”€â”€ LayerNorm
â”‚   â”œâ”€â”€ Feed-Forward Network (n_embd â†’ 4Ã—n_embd â†’ n_embd)
â”‚   â””â”€â”€ Residual Connection
â”œâ”€â”€ Final LayerNorm
â””â”€â”€ Language Model Head (n_embd â†’ vocab_size)
```

**Default Hyperparameters:**
- Embedding dimension: 384
- Number of layers: 6
- Number of attention heads: 6
- Block size (context length): 256
- Dropout: 0.2

## ğŸ”§ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/mini-gpt.git
cd mini-gpt

# Install dependencies
pip install -r requirements.txt
```

**Requirements:**
- Python 3.7+
- PyTorch
- tqdm
- numpy

## ğŸ¯ How to Train

1. **Prepare the data**: The Tiny Shakespeare dataset should be in `data/input.txt` (already included)

2. **Start training**:
```bash
python train.py
```

Training progress will be displayed with a progress bar. The model will:
- Evaluate train/val loss every 500 iterations
- Save the final checkpoint to `ckpt.pt`
- Generate a sample text at the end

**Training output example:**
```
Loading data...
Dataset length: 1115394 characters
Vocabulary size: 65
Model parameters: 10,788,929

Starting training for 5000 iterations...
Step 0: train loss 4.1745, val loss 4.1692
Step 500: train loss 1.9856, val loss 2.0134
...
```

## ğŸ¨ How to Generate Text

After training, use `generate.py` to create new text:

```bash
# Generate with default settings
python generate.py

# Generate with a custom prompt
python generate.py --prompt "ROMEO:" --max_new_tokens 300

# Adjust creativity (temperature)
python generate.py --prompt "To be or not to be" --temperature 1.0 --top_k 100

# All available options
python generate.py \
    --prompt "Hello" \
    --max_new_tokens 200 \
    --temperature 0.8 \
    --top_k 200 \
    --checkpoint ckpt.pt \
    --device cpu
```

**Parameters:**
- `--prompt`: Starting text (empty for random generation)
- `--max_new_tokens`: Number of new tokens to generate
- `--temperature`: Sampling temperature (higher = more random, lower = more deterministic)
- `--top_k`: Consider only top-k most likely tokens
- `--checkpoint`: Path to saved model checkpoint
- `--device`: Use 'cuda' for GPU or 'cpu'

**Example output:**
```
GENERATING TEXT
============================================================
Prompt: ROMEO:
Max tokens: 300
Temperature: 0.8
Top-k: 200
============================================================

ROMEO:
What shall I do, but with the world's consent,
That I should love thee, and be thy friend?
...
```

## ğŸ“ Project Structure

```
mini-gpt/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.txt              # Tiny Shakespeare dataset
â”‚
â”œâ”€â”€ model.py                   # GPT model architecture
â”œâ”€â”€ train.py                   # Training script
â”œâ”€â”€ generate.py                # Text generation script
â”œâ”€â”€ utils.py                   # Character tokenizer
â”œâ”€â”€ config.py                  # Hyperparameters
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ ckpt.pt                    # Saved model (after training)
â”‚
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ TROUBLESHOOTING.md         # Common issues and solutions
â””â”€â”€ FUTURE_WORK.md             # Planned improvements
```

## ğŸ§  Key Learnings

This project demonstrates several fundamental concepts in modern NLP:

1. **Self-Attention Mechanism**: How tokens "attend" to previous tokens in the sequence using Q, K, V matrices
2. **Causal Masking**: Preventing the model from "looking ahead" during training
3. **Residual Streams**: Skip connections that allow gradients to flow more easily
4. **Layer Normalization**: Stabilizing training by normalizing activations
5. **Autoregressive Generation**: Sampling tokens one at a time, conditioning on previous tokens
6. **Character-Level Modeling**: Understanding text at the character granularity

## ğŸ”® Future Improvements

See `FUTURE_WORK.md` for detailed plans. Key areas:

- **BPE Tokenizer**: Implement Byte-Pair Encoding for more efficient tokenization
- **Weights & Biases Integration**: Add experiment tracking and visualization
- **Multi-GPU Support**: Distributed training for larger models
- **Chinese Dataset**: Train on Chinese text (e.g., Chinese poetry)
- **KV Caching**: Optimize generation speed by caching key/value states
- **Flash Attention**: More efficient attention computation
- **Learning Rate Scheduling**: Cosine annealing, warmup
- **Gradient Clipping**: Prevent gradient explosion

## ğŸ“ References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 paper
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathy's minimal GPT implementation
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual guide to Transformers

## ğŸ“„ License

MIT License - feel free to use this code for learning and experimentation!

---

## ğŸ’¼ LinkedIn Project Summary

**Copy-friendly short description for LinkedIn:**

```
Mini-GPT: A nanoGPT-style Transformer built entirely from scratch using PyTorch.
Includes tokenizer, training pipeline, autoregressive generation, and full project setup.

âœ… Decoder-only Transformer architecture
âœ… Multi-head self-attention with causal masking
âœ… Character-level tokenization
âœ… Training on Tiny Shakespeare dataset
âœ… Text generation with temperature & top-k sampling
âœ… Complete documentation and reproducible setup

Skills: PyTorch, Transformers, NLP, LLMs, Deep Learning, Python

GitHub: [your-repo-link]
```

---

**Built with â¤ï¸ for learning and understanding language models**
