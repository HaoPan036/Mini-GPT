# ðŸ› ï¸ Mini-GPT å®žè·µæ•™ç¨‹ - ä¸€æ­¥ä¸€æ­¥åŠ¨æ‰‹åš

## ðŸŽ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡å®žé™…æ“ä½œï¼Œæ·±å…¥ç†è§£ Mini-GPT çš„æ¯ä¸ªç»„ä»¶ã€‚

---

# ç¬¬ä¸€å¤©ï¼šç†è§£åˆ†è¯å™¨ï¼ˆ30åˆ†é’Ÿï¼‰

## æ­¥éª¤1ï¼šåˆ›å»ºæµ‹è¯•æ–‡ä»¶

```bash
cd /Users/hao/Desktop/CS336/mini-gpt
```

## æ­¥éª¤2ï¼šåˆ›å»ºå¹¶è¿è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•

åˆ›å»ºæ–‡ä»¶ `day1_tokenizer.py`ï¼š

```python
# ===== å®žéªŒ1ï¼šåŸºç¡€åˆ†è¯å™¨æµ‹è¯• =====
from utils import CharTokenizer

# ç®€å•çš„è‹±æ–‡æ–‡æœ¬
text = "hello world"
print("=" * 50)
print("å®žéªŒ1ï¼šè‹±æ–‡åˆ†è¯å™¨")
print("=" * 50)
print(f"åŽŸå§‹æ–‡æœ¬: {text}")

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = CharTokenizer(text)

# æŸ¥çœ‹è¯æ±‡è¡¨
print(f"\nè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
print(f"æ‰€æœ‰å­—ç¬¦: {sorted(tokenizer.stoi.keys())}")
print(f"\nå­—ç¬¦â†’æ•°å­—æ˜ å°„:")
for char, idx in sorted(tokenizer.stoi.items()):
    print(f"  '{char}' â†’ {idx}")

# æµ‹è¯•ç¼–ç 
test_word = "hello"
encoded = tokenizer.encode(test_word)
print(f"\nç¼–ç  '{test_word}': {encoded}")

# æµ‹è¯•è§£ç 
decoded = tokenizer.decode(encoded)
print(f"è§£ç å›žæ¥: '{decoded}'")

# éªŒè¯æ­£ç¡®æ€§
print(f"\néªŒè¯: {test_word == decoded} âœ“" if test_word == decoded else f"âœ— é”™è¯¯ï¼")

print("\n" + "=" * 50)
print("å®žéªŒ2ï¼šä¸­è‹±æ–‡æ··åˆ")
print("=" * 50)

# ä¸­è‹±æ–‡æ··åˆ
mixed_text = "Hello ä½ å¥½"
tokenizer2 = CharTokenizer(mixed_text)
print(f"åŽŸå§‹æ–‡æœ¬: {mixed_text}")
print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer2.vocab_size}")
print(f"å­—ç¬¦é›†: {sorted(tokenizer2.stoi.keys())}")

encoded_mixed = tokenizer2.encode(mixed_text)
decoded_mixed = tokenizer2.decode(encoded_mixed)
print(f"\nç¼–ç : {encoded_mixed}")
print(f"è§£ç : '{decoded_mixed}'")

print("\n" + "=" * 50)
print("å®žéªŒ3ï¼šæ•°æ®é›†ç»Ÿè®¡")
print("=" * 50)

# åŠ è½½å®žé™…æ•°æ®é›†
with open('data/input.txt', 'r', encoding='utf-8') as f:
    shakespeare = f.read()

print(f"èŽŽå£«æ¯”äºšæ–‡æœ¬é•¿åº¦: {len(shakespeare):,} å­—ç¬¦")
tokenizer3 = CharTokenizer(shakespeare)
print(f"å”¯ä¸€å­—ç¬¦æ•°: {tokenizer3.vocab_size}")
print(f"å‰100ä¸ªå­—ç¬¦: {shakespeare[:100]}")

# ç¼–ç å‰10ä¸ªå­—ç¬¦
sample = shakespeare[:10]
sample_encoded = tokenizer3.encode(sample)
print(f"\næ ·æœ¬: '{sample}'")
print(f"ç¼–ç : {sample_encoded}")
print(f"è§£ç : '{tokenizer3.decode(sample_encoded)}'")
```

**è¿è¡Œ**ï¼š
```bash
python3 day1_tokenizer.py
```

## æ­¥éª¤3ï¼šæ€è€ƒä¸Žè®°å½•

åœ¨ç¬”è®°æœ¬è®°å½•ï¼š
1. è¯æ±‡è¡¨å¤§å°å¯¹ä»€ä¹ˆæœ‰å½±å“ï¼Ÿ
2. ä¸ºä»€ä¹ˆä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å­—ç¬¦æ··åˆæ—¶è¯æ±‡è¡¨ä¼šå˜å¤§ï¼Ÿ
3. å¦‚æžœé‡åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„å­—ç¬¦ä¼šæ€Žæ ·ï¼Ÿï¼ˆè¯•è¯• `tokenizer.encode("xyz123")`ï¼‰

---

# ç¬¬äºŒå¤©ï¼šç†è§£ Embeddingï¼ˆ40åˆ†é’Ÿï¼‰

## æ­¥éª¤1ï¼šåˆ›å»º Embedding å®žéªŒ

åˆ›å»ºæ–‡ä»¶ `day2_embedding.py`ï¼š

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

print("=" * 50)
print("å®žéªŒ1ï¼šToken Embedding åŸºç¡€")
print("=" * 50)

# åˆ›å»ºä¸€ä¸ªå°åž‹ embedding
vocab_size = 10  # 10ä¸ªä¸åŒçš„token
embed_dim = 4    # æ¯ä¸ªtokenç”¨4ç»´å‘é‡è¡¨ç¤º

embedding = nn.Embedding(vocab_size, embed_dim)

# æŸ¥çœ‹embeddingçŸ©é˜µ
print(f"Embedding çŸ©é˜µå½¢çŠ¶: {embedding.weight.shape}")
print(f"è¿™æ˜¯ä¸€ä¸ª {vocab_size} Ã— {embed_dim} çš„çŸ©é˜µ\n")

# æµ‹è¯•: å°† token 0 è½¬æ¢æˆå‘é‡
token_0 = torch.tensor([0])
vec_0 = embedding(token_0)
print(f"Token 0 çš„å‘é‡: {vec_0.squeeze().detach().numpy()}")

# æµ‹è¯•: å°†å¤šä¸ª tokens è½¬æ¢
tokens = torch.tensor([0, 1, 2, 0])  # ä¸€ä¸ªåºåˆ—
vecs = embedding(tokens)
print(f"\nåºåˆ— {tokens.tolist()} çš„å‘é‡çŸ©é˜µå½¢çŠ¶: {vecs.shape}")
print(f"è¿™è¡¨ç¤º 4ä¸ªtokenï¼Œæ¯ä¸ªç”¨4ç»´å‘é‡")

print("\n" + "=" * 50)
print("å®žéªŒ2ï¼šç›¸ä¼¼åº¦è®¡ç®—")
print("=" * 50)

# è®¡ç®—ä¸¤ä¸ªtokençš„ç›¸ä¼¼åº¦
vec_0 = embedding(torch.tensor([0])).squeeze()
vec_1 = embedding(torch.tensor([1])).squeeze()

# ä½™å¼¦ç›¸ä¼¼åº¦
cos_sim = torch.cosine_similarity(vec_0, vec_1, dim=0)
print(f"Token 0 å’Œ Token 1 çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cos_sim.item():.4f}")
print("ç›¸ä¼¼åº¦è¶ŠæŽ¥è¿‘1ï¼Œè¡¨ç¤ºè¶Šç›¸ä¼¼")

print("\n" + "=" * 50)
print("å®žéªŒ3ï¼šPosition Embedding")
print("=" * 50)

# ä½ç½®ç¼–ç 
max_len = 8
pos_embedding = nn.Embedding(max_len, embed_dim)

# åºåˆ—çš„ä½ç½®
positions = torch.arange(max_len)
pos_vecs = pos_embedding(positions)

print(f"8ä¸ªä½ç½®çš„ç¼–ç çŸ©é˜µå½¢çŠ¶: {pos_vecs.shape}")
print(f"\nä½ç½®0çš„å‘é‡: {pos_vecs[0].detach().numpy()}")
print(f"ä½ç½®1çš„å‘é‡: {pos_vecs[1].detach().numpy()}")

print("\n" + "=" * 50)
print("å®žéªŒ4ï¼šå®Œæ•´è¾“å…¥ = Token + Position")
print("=" * 50)

# ä¸€ä¸ªä¾‹å­åºåˆ—
sequence = torch.tensor([3, 1, 4, 1])
seq_len = len(sequence)

# Token embeddings
token_emb = embedding(sequence)
print(f"Token embeddings å½¢çŠ¶: {token_emb.shape}")

# Position embeddings
positions = torch.arange(seq_len)
pos_emb = pos_embedding(positions)
print(f"Position embeddings å½¢çŠ¶: {pos_emb.shape}")

# ç›¸åŠ 
final_input = token_emb + pos_emb
print(f"æœ€ç»ˆè¾“å…¥å½¢çŠ¶: {final_input.shape}")
print("\nè¿™å°±æ˜¯è¾“å…¥åˆ° Transformer çš„æ•°æ®ï¼")

# å¯è§†åŒ–ï¼ˆå¦‚æžœæœ‰ matplotlibï¼‰
try:
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 3, 1)
    plt.imshow(token_emb.detach().numpy(), aspect='auto', cmap='coolwarm')
    plt.title('Token Embeddings')
    plt.ylabel('Sequence Position')
    plt.xlabel('Embedding Dimension')
    plt.colorbar()
    
    plt.subplot(1, 3, 2)
    plt.imshow(pos_emb.detach().numpy(), aspect='auto', cmap='coolwarm')
    plt.title('Position Embeddings')
    plt.xlabel('Embedding Dimension')
    plt.colorbar()
    
    plt.subplot(1, 3, 3)
    plt.imshow(final_input.detach().numpy(), aspect='auto', cmap='coolwarm')
    plt.title('Final Input (Token + Position)')
    plt.xlabel('Embedding Dimension')
    plt.colorbar()
    
    plt.tight_layout()
    plt.savefig('day2_embeddings.png')
    print("\nâœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ° day2_embeddings.png")
except:
    print("\n(å¯è§†åŒ–éœ€è¦ matplotlib)")
```

**è¿è¡Œ**ï¼š
```bash
python3 day2_embedding.py
```

---

# ç¬¬ä¸‰å¤©ï¼šç†è§£æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ60åˆ†é’Ÿï¼‰

## æ­¥éª¤1ï¼šæ‰‹åŠ¨è®¡ç®—æ³¨æ„åŠ›

åˆ›å»ºæ–‡ä»¶ `day3_attention.py`ï¼š

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

print("=" * 60)
print("æ‰‹æŠŠæ‰‹ç†è§£æ³¨æ„åŠ›æœºåˆ¶")
print("=" * 60)

# ===== ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ€ç®€å•çš„æ³¨æ„åŠ›ä¾‹å­ =====
print("\nã€ç¬¬1æ­¥ã€‘åˆ›å»ºä¸€ä¸ªç®€å•çš„åºåˆ—")
print("-" * 60)

# å‡è®¾æˆ‘ä»¬æœ‰3ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨2ç»´å‘é‡è¡¨ç¤º
sequence = torch.tensor([
    [1.0, 0.0],  # è¯1
    [0.0, 1.0],  # è¯2
    [0.5, 0.5],  # è¯3
])

print(f"åºåˆ—å½¢çŠ¶: {sequence.shape}")  # (3, 2)
print(f"3ä¸ªè¯ï¼Œæ¯ä¸ªè¯2ç»´\n")
print("åºåˆ—å†…å®¹:")
print(sequence)

# ===== ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›å»º Q, K, V =====
print("\nã€ç¬¬2æ­¥ã€‘åˆ›å»º Query, Key, Value")
print("-" * 60)

# ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æŽ¥ä½¿ç”¨åºåˆ—æœ¬èº«
Q = sequence  # Query: "æˆ‘è¦æ‰¾ä»€ä¹ˆ"
K = sequence  # Key: "æˆ‘æœ‰ä»€ä¹ˆ"
V = sequence  # Value: "å†…å®¹æ˜¯ä»€ä¹ˆ"

print("åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒQ = K = V = sequence")
print("(å®žé™…ä¸­ä¼šç”¨çº¿æ€§å±‚å˜æ¢)")

# ===== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° =====
print("\nã€ç¬¬3æ­¥ã€‘è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°")
print("-" * 60)

# Q @ K^T
scores = Q @ K.transpose(0, 1)
print(f"æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µå½¢çŠ¶: {scores.shape}")  # (3, 3)
print("\næ³¨æ„åŠ›åˆ†æ•° (Q @ K^T):")
print(scores.numpy())
print("\nè§£é‡Š:")
print("scores[i][j] = è¯iå¯¹è¯jçš„å…³æ³¨ç¨‹åº¦")

# ç¼©æ”¾
d_k = Q.shape[-1]  # ç»´åº¦
scores_scaled = scores / (d_k ** 0.5)
print(f"\nç¼©æ”¾åŽ (é™¤ä»¥ âˆš{d_k}):")
print(scores_scaled.numpy())

# Softmax è½¬æ¢æˆæ¦‚çŽ‡
attention_weights = F.softmax(scores_scaled, dim=-1)
print("\nSoftmax åŽ (è½¬æ¢æˆæ¦‚çŽ‡):")
print(attention_weights.numpy())
print("\næ¯ä¸€è¡Œçš„å’Œ:", attention_weights.sum(dim=-1).numpy())
print("âœ“ æ¯è¡Œå’Œä¸º1ï¼Œè¿™å°±æ˜¯æ¦‚çŽ‡åˆ†å¸ƒï¼")

# ===== ç¬¬å››éƒ¨åˆ†ï¼šåº”ç”¨æ³¨æ„åŠ› =====
print("\nã€ç¬¬4æ­¥ã€‘åº”ç”¨æ³¨æ„åŠ›åˆ° Value")
print("-" * 60)

output = attention_weights @ V
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print("\nè¾“å‡º:")
print(output.numpy())
print("\nè¿™å°±æ˜¯æ³¨æ„åŠ›çš„ç»“æžœï¼")
print("æ¯ä¸ªè¯çš„è¾“å‡º = æ‰€æœ‰è¯çš„åŠ æƒå¹³å‡")

# ===== ç¬¬äº”éƒ¨åˆ†ï¼šå› æžœæŽ©ç  =====
print("\nã€ç¬¬5æ­¥ã€‘å› æžœæŽ©ç  (Causal Mask)")
print("-" * 60)

# åˆ›å»ºå› æžœæŽ©ç 
mask = torch.tril(torch.ones(3, 3))
print("å› æžœæŽ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ):")
print(mask.numpy())
print("\n1è¡¨ç¤ºå¯ä»¥çœ‹ï¼Œ0è¡¨ç¤ºä¸èƒ½çœ‹")

# åº”ç”¨æŽ©ç 
scores_masked = scores_scaled.masked_fill(mask == 0, float('-inf'))
print("\nåº”ç”¨æŽ©ç åŽ:")
print(scores_masked.numpy())
print("\n-inf çš„ä½ç½®åœ¨ softmax åŽä¼šå˜æˆ0")

# Softmax
attention_weights_masked = F.softmax(scores_masked, dim=-1)
print("\nSoftmax åŽ:")
print(attention_weights_masked.numpy())
print("\nè§‚å¯Ÿ:")
print("- ç¬¬1è¡Œ: åªèƒ½çœ‹è‡ªå·±")
print("- ç¬¬2è¡Œ: å¯ä»¥çœ‹å‰2ä¸ªè¯")
print("- ç¬¬3è¡Œ: å¯ä»¥çœ‹æ‰€æœ‰3ä¸ªè¯")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# æ— æŽ©ç çš„æ³¨æ„åŠ›
im1 = axes[0].imshow(attention_weights.numpy(), cmap='Blues', vmin=0, vmax=1)
axes[0].set_title('Attention Weights (No Mask)')
axes[0].set_xlabel('Key Position')
axes[0].set_ylabel('Query Position')
plt.colorbar(im1, ax=axes[0])

# æœ‰æŽ©ç çš„æ³¨æ„åŠ›
im2 = axes[1].imshow(attention_weights_masked.numpy(), cmap='Blues', vmin=0, vmax=1)
axes[1].set_title('Attention Weights (Causal Mask)')
axes[1].set_xlabel('Key Position')
axes[1].set_ylabel('Query Position')
plt.colorbar(im2, ax=axes[1])

plt.tight_layout()
plt.savefig('day3_attention.png')
print("\nâœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ° day3_attention.png")

# ===== ç¬¬å…­éƒ¨åˆ†ï¼šå®žé™…æ¨¡åž‹çš„æ³¨æ„åŠ› =====
print("\n" + "=" * 60)
print("ç”¨çœŸå®žæ¨¡åž‹çš„æ³¨æ„åŠ›å¤´")
print("=" * 60)

from model import Head

# åˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›å¤´
n_embd = 32
head_size = 8
block_size = 10

head = Head(n_embd, head_size, block_size)

# åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥ (batch=1, seq_len=5, embed_dim=32)
x = torch.randn(1, 5, n_embd)
print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")

# å‰å‘ä¼ æ’­
output = head(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print("\nâœ“ æ³¨æ„åŠ›å¤´æˆåŠŸè¿è¡Œï¼")

print("\n" + "=" * 60)
print("æ€»ç»“")
print("=" * 60)
print("""
æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ­¥éª¤ï¼š

1. è¾“å…¥åºåˆ— â†’ Q, K, V (é€šè¿‡çº¿æ€§å˜æ¢)
2. è®¡ç®—ç›¸ä¼¼åº¦: Q @ K^T
3. ç¼©æ”¾: / âˆšd_k
4. åº”ç”¨æŽ©ç  (å¯¹äºŽå› æžœæ³¨æ„åŠ›)
5. Softmax: è½¬æ¢æˆæ¦‚çŽ‡
6. åŠ æƒæ±‚å’Œ: @ V
7. è¾“å‡ºç»“æžœ

å…³é”®ç†è§£ï¼š
- æ³¨æ„åŠ›å°±æ˜¯"åŠ æƒå¹³å‡"
- æƒé‡æ¥è‡ªç›¸ä¼¼åº¦è®¡ç®—
- å› æžœæŽ©ç é˜²æ­¢çœ‹åˆ°æœªæ¥
""")
```

**è¿è¡Œ**ï¼š
```bash
python3 day3_attention.py
```

---

# ç¬¬å››å¤©ï¼šè®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªæ¨¡åž‹ï¼ˆ90åˆ†é’Ÿï¼‰

## æ­¥éª¤1ï¼šå°è§„æ¨¡å¿«é€Ÿå®žéªŒ

åˆ›å»ºæ–‡ä»¶ `day4_train_small.py`ï¼š

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
from tqdm import tqdm

print("=" * 60)
print("è®­ç»ƒä¸€ä¸ªè¶…å°åž‹æ¨¡åž‹ï¼ˆ5åˆ†é’Ÿå†…å®Œæˆï¼‰")
print("=" * 60)

# ===== é…ç½® =====
# è¶…å°å‚æ•°ï¼Œå¿«é€Ÿçœ‹åˆ°æ•ˆæžœ
batch_size = 16
block_size = 32
max_iters = 500  # åªè®­ç»ƒ500æ­¥
eval_interval = 100
learning_rate = 1e-3

# æ¨¡åž‹å‚æ•°ï¼ˆå¾ˆå°ï¼‰
n_embd = 64
num_heads = 2
n_layer = 2
dropout = 0.1

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ===== åŠ è½½æ•°æ® =====
with open('data/input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# ä½¿ç”¨å‰10000ä¸ªå­—ç¬¦ï¼ˆæ›´å¿«ï¼‰
text = text[:10000]
print(f"\næ•°æ®é•¿åº¦: {len(text)} å­—ç¬¦")

# åˆ†è¯å™¨
from utils import CharTokenizer
tokenizer = CharTokenizer(text)
vocab_size = tokenizer.vocab_size
print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

# ç¼–ç 
data = torch.tensor(tokenizer.encode(text), dtype=torch.long)

# åˆ†å‰²
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

print(f"è®­ç»ƒé›†: {len(train_data)} tokens")
print(f"éªŒè¯é›†: {len(val_data)} tokens")

# ===== æ•°æ®åŠ è½½å‡½æ•° =====
def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

# ===== åˆ›å»ºæ¨¡åž‹ =====
from model import GPTModel

model = GPTModel(
    vocab_size=vocab_size,
    n_embd=n_embd,
    num_heads=num_heads,
    n_layer=n_layer,
    block_size=block_size,
    dropout=dropout
)
model = model.to(device)

# ç»Ÿè®¡å‚æ•°
n_params = sum(p.numel() for p in model.parameters())
print(f"\næ¨¡åž‹å‚æ•°: {n_params:,}")

# ===== ä¼˜åŒ–å™¨ =====
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# ===== è¯„ä¼°å‡½æ•° =====
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(20)  # å°‘é‡è¯„ä¼°ï¼Œæ›´å¿«
        for k in range(20):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# ===== è®­ç»ƒå¾ªçŽ¯ =====
print("\n" + "=" * 60)
print("å¼€å§‹è®­ç»ƒ")
print("=" * 60)

losses_history = {'train': [], 'val': []}

for iter in tqdm(range(max_iters), desc="è®­ç»ƒä¸­"):
    # è¯„ä¼°
    if iter % eval_interval == 0 or iter == max_iters - 1:
        losses = estimate_loss()
        losses_history['train'].append(losses['train'])
        losses_history['val'].append(losses['val'])
        print(f"\nStep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
    
    # è®­ç»ƒæ­¥éª¤
    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

print("\nâœ“ è®­ç»ƒå®Œæˆï¼")

# ===== ç”Ÿæˆæ ·æœ¬ =====
print("\n" + "=" * 60)
print("ç”Ÿæˆæ ·æœ¬æ–‡æœ¬")
print("=" * 60)

model.eval()
context = torch.zeros((1, 1), dtype=torch.long, device=device)
generated = model.generate(context, max_new_tokens=200, temperature=0.8, top_k=50)
generated_text = tokenizer.decode(generated[0].tolist())

print("\nç”Ÿæˆçš„æ–‡æœ¬:")
print("-" * 60)
print(generated_text)
print("-" * 60)

# ===== å¯è§†åŒ– loss =====
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(losses_history['train'], label='Train Loss', marker='o')
plt.plot(losses_history['val'], label='Val Loss', marker='s')
plt.xlabel('Evaluation Step')
plt.ylabel('Loss')
plt.title('Training Progress')
plt.legend()
plt.grid(True)
plt.savefig('day4_training_curve.png')
print("\nâœ“ Loss æ›²çº¿å·²ä¿å­˜åˆ° day4_training_curve.png")

# ===== ä¿å­˜æ¨¡åž‹ =====
torch.save({
    'model_state_dict': model.state_dict(),
    'tokenizer_stoi': tokenizer.stoi,
    'tokenizer_itos': tokenizer.itos,
    'vocab_size': vocab_size,
    'config': {
        'n_embd': n_embd,
        'num_heads': num_heads,
        'n_layer': n_layer,
        'block_size': block_size,
    }
}, 'day4_model.pt')

print("âœ“ æ¨¡åž‹å·²ä¿å­˜åˆ° day4_model.pt")

print("\n" + "=" * 60)
print("å®žéªŒæ€»ç»“")
print("=" * 60)
print(f"""
è®­ç»ƒé…ç½®:
- æ•°æ®é‡: {len(text)} å­—ç¬¦
- è®­ç»ƒæ­¥æ•°: {max_iters}
- æ¨¡åž‹å‚æ•°: {n_params:,}
- æœ€ç»ˆè®­ç»ƒ loss: {losses_history['train'][-1]:.4f}
- æœ€ç»ˆéªŒè¯ loss: {losses_history['val'][-1]:.4f}

è§‚å¯Ÿ:
1. Loss æ˜¯å¦ä¸‹é™ï¼Ÿ
2. ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦æœ‰æ”¹å–„ï¼Ÿ
3. è®­ç»ƒ loss å’ŒéªŒè¯ loss çš„å…³ç³»ï¼Ÿ

ä¸‹ä¸€æ­¥:
1. å¢žåŠ è®­ç»ƒæ­¥æ•° (max_iters = 2000)
2. å¢žåŠ æ¨¡åž‹å¤§å° (n_embd = 128, n_layer = 4)
3. ä½¿ç”¨å®Œæ•´æ•°æ®é›†
""")
```

**è¿è¡Œ**ï¼š
```bash
python3 day4_train_small.py
```

è¿™å°†åœ¨5-10åˆ†é’Ÿå†…å®Œæˆï¼

---

# ç¬¬äº”å¤©ï¼šå®Œæ•´è®­ç»ƒä¸Žå®žéªŒï¼ˆè‡ªä¸»æŽ¢ç´¢ï¼‰

## å®žéªŒæ¸…å•

### âœ… å®žéªŒ1ï¼šä¸åŒæ¨¡åž‹å¤§å°

ä¿®æ”¹ `config.py`ï¼Œå°è¯•ï¼š

```python
# æžå°æ¨¡åž‹
n_embd = 64
n_layer = 2
num_heads = 2

# å°æ¨¡åž‹
n_embd = 128
n_layer = 4
num_heads = 4

# é»˜è®¤æ¨¡åž‹
n_embd = 384
n_layer = 6
num_heads = 6
```

**è®°å½•**ï¼šå‚æ•°é‡ã€è®­ç»ƒæ—¶é—´ã€æœ€ç»ˆ loss

### âœ… å®žéªŒ2ï¼šä¸åŒå­¦ä¹ çŽ‡

```python
learning_rate_options = [1e-5, 3e-4, 1e-3, 3e-3]
```

**è§‚å¯Ÿ**ï¼šå“ªä¸ªå­¦ä¹ çŽ‡æ”¶æ•›æœ€å¿«ï¼Ÿ

### âœ… å®žéªŒ3ï¼šç”Ÿæˆå‚æ•°

```bash
# Temperature
python3 generate.py --temperature 0.3  # ä¿å®ˆ
python3 generate.py --temperature 1.0  # å¹³è¡¡
python3 generate.py --temperature 1.5  # åˆ›æ–°

# Top-K
python3 generate.py --top_k 10   # é™åˆ¶é€‰æ‹©
python3 generate.py --top_k 200  # æ›´å¤šé€‰æ‹©
```

**æ¯”è¾ƒ**ï¼šè¾“å‡ºçš„è´¨é‡å’Œå¤šæ ·æ€§

### âœ… å®žéªŒ4ï¼šè‡ªå®šä¹‰æ•°æ®é›†

åˆ›å»ºä½ è‡ªå·±çš„æ–‡æœ¬æ–‡ä»¶ï¼š

```bash
# ä¸­æ–‡æ–‡æœ¬
cat > data/chinese.txt << 'END'
åºŠå‰æ˜Žæœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚
ä¸¾å¤´æœ›æ˜Žæœˆï¼Œä½Žå¤´æ€æ•…ä¹¡ã€‚
...
END

# ä¿®æ”¹ train.py ä¸­çš„æ•°æ®è·¯å¾„
# è®­ç»ƒ
python3 train.py
```

---

# å­¦ä¹ æ£€æŸ¥ç‚¹

## Week 1 Checkpoint

- [ ] èƒ½è§£é‡Šåˆ†è¯å™¨çš„ä½œç”¨
- [ ] ç†è§£ stoi å’Œ itos çš„åŒºåˆ«
- [ ] èƒ½æ‰‹åŠ¨ç¼–ç /è§£ç ä¸€æ®µæ–‡å­—

## Week 2 Checkpoint

- [ ] ç†è§£ Embedding çš„æ¦‚å¿µ
- [ ] çŸ¥é“ Position Embedding çš„ä½œç”¨
- [ ] èƒ½ç”»å‡ºæ³¨æ„åŠ›æœºåˆ¶çš„æµç¨‹å›¾

## Week 3 Checkpoint

- [ ] æˆåŠŸè¿è¡Œä¸€æ¬¡å®Œæ•´è®­ç»ƒ
- [ ] ç†è§£è®­ç»ƒå¾ªçŽ¯çš„æ¯ä¸ªæ­¥éª¤
- [ ] èƒ½è§£é‡Š loss ä¸‹é™çš„å«ä¹‰

## Week 4 Checkpoint

- [ ] å°è¯•è¿‡è‡³å°‘3ç§ä¸åŒé…ç½®
- [ ] èƒ½ç‹¬ç«‹è°ƒè¯•å¸¸è§é”™è¯¯
- [ ] ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡å¯æŽ¥å—

---

# ä¸‹ä¸€æ­¥ï¼šè¿›é˜¶é¡¹ç›®

## é¡¹ç›®1ï¼šå¯è§†åŒ–å·¥å…·

åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çš„å·¥å…·

## é¡¹ç›®2ï¼šå¯¹è¯æ¨¡åž‹

è®­ç»ƒä¸€ä¸ªç®€å•çš„é—®ç­”æ¨¡åž‹

## é¡¹ç›®3ï¼šä»£ç ç”Ÿæˆ

åœ¨ä»£ç æ•°æ®é›†ä¸Šè®­ç»ƒ

## é¡¹ç›®4ï¼šæ€§èƒ½ä¼˜åŒ–

å®žçŽ° KV cacheï¼ŒåŠ é€Ÿç”Ÿæˆ

---

**è®°ä½ï¼šå®žè·µæ˜¯æœ€å¥½çš„è€å¸ˆï¼åŠ¨æ‰‹åšï¼Œå¤šå®žéªŒï¼** ðŸš€
